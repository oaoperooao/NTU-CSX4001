---
title: "UT-TF"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 載入資料庫
```{R library}
# Import libraries
library(NLP)
library(readtext)
library(tm)
library(tidyverse)      # opinionated collection of R packages designed for data science
library(stringr)        # text cleaning and regular expression
library(dplyr)
library(jiebaR)
library(tidytext)
library(wordcloud2)
library(fpc)  
```

##載入資料
```{r}
# Text tidying!
raw <- read_html("https://www.metacritic.com/game/playstation-4/undertale/critic-reviews")
rawPSC <-html_nodes(raw," .review_body")%>%html_text(raw)

raw <- read_html("https://www.metacritic.com/game/playstation-4/undertale/user-reviews")
rawPSU <-html_nodes(raw," .review_body")%>%html_text(raw)


raw <- read_html("https://www.metacritic.com/game/switch/undertale/critic-reviews")
rawNSC <-html_nodes(raw," .review_body")%>%html_text(raw)


raw <- read_html("https://www.metacritic.com/game/switch/undertale/user-reviews")
rawNSU <-html_nodes(raw," .review_body")%>%html_text(raw)

raw <- read_html("https://www.metacritic.com/game/pc/undertale/critic-reviews")
rawPCC <-html_nodes(raw," .review_body")%>%html_text(raw)


raw <- read_html("https://www.metacritic.com/game/pc/undertale/user-reviews")
rawPCU <-html_nodes(raw," .review_body")%>%html_text(raw)

list.rating <- list(rawPSC,rawPSU,rawNSC,rawNSU,rawPCC,rawPCU)
titles <- c("PS4.critic","PS4.user","NS.critic","NS.user","PC.critic","PC.user")
```

## 定義標題
```{r}
titles <- c("Reddit", "Twitter","Tumblr","Facebook","Pinterest","Line","Flickr")

# store the content of the website
websites_content <- list(reddit,twitter,tumblr,fb,pin,line,flickr)
```

##整理資料
```{r}
# define an empty data frame as comparison
comparison <- tibble()

for(i in seq_along(titles)) { 
  # create from column vectors
        clean <- tibble(site = seq_along(websites_content[[i]]),
                        text = websites_content[[i]]) %>%
             unnest_tokens(word, text) %>%      # seperate the content into words
          # add a new column
             mutate(websites_content = titles[i]) %>%
          # choose the column
             select(websites_content, everything())
        # add together! to a single data frame
        comparison <- rbind(comparison, clean)
}

colnames(comparison) <- c("websites_title", "site", "word")


comparison
comparison$websites_title <- factor(comparison$websites_title, levels = rev(titles))
```

##找出詞頻
```{r}
# now ~ find the word frequency
# smart~ it has stio_words build in~ just remove those word
# the first aproach!
comparison_word_freq <- comparison %>%
        anti_join(stop_words) %>%
        count(word, sort = TRUE) %>%
        filter(n > 20)
comparison_word_freq
```

##製成文字雲
    可以發現，在三個網站中，最常使用的字是information，便是告知使用者那些資訊會被蒐集，而services站第二位，可見蒐集資料是為了提供服務，而account,privacy,data,content等等，皆和蒐集的內容有關，再小一點的字還有cookies,browser,address，可以得知網站蒐集的資料細項。
```{r}
wordcloud2(data = comparison_word_freq, size=1)
```


## 製成bar chart
    在下面可見大部分使用的詞，莫過於information,services,網站名稱(twitter,tumblr,reddit),privacy等等。其中由一個看似無關的詞，party，其實只是third party(第三方網站)的縮寫(還有被切斷的，我上課再問一下)。
```{r}
# Find the most frequently used word in each website
comparison_word_freq_each_website <- comparison %>%
        anti_join(stop_words) %>%
        group_by(websites_title) %>%
        count(word, sort = TRUE) %>%
        top_n(10) %>%
        ungroup() %>%
        mutate(website = factor(websites_title),
               text_order = nrow(.):1) %>%
        ggplot(aes(reorder(word, text_order), n, fill = website)) +
          geom_bar(stat = "identity") +
          facet_wrap(~ website, scales = "free_y") +
          labs(x = "NULL", y = "Frequency") +
          coord_flip() +
          theme(legend.position="none")
comparison_word_freq_each_website
```


#計算單字佔文章百分率(單字出現率)
    以information,services,account占前三名，而各網站最常出現的前三名是

Reddit:information,services,reddit

Twitter:twitter,information,data

Tumblr:information,services,account

可見即使是相同的隱私權政策，三個網站的注重點仍然不同。reddit和大體相同，走平穩路線；twitter則是搜集較多data(或是只是想代換詞句？)；tumblr的隱私權，最主要和帳戶相關資料講得比較詳細。
```{r}
# calculate percent of word use across all websites
site_pct <- comparison %>%
        anti_join(stop_words) %>%
        count(word) %>%
        transmute(word, all_words = n / sum(n))

# calculate percent of word use within each website
frequency <- comparison %>%
        anti_join(stop_words) %>%
        count(websites_title, word) %>%
        mutate(website_words = n / sum(n)) %>%
        left_join(site_pct) %>%
        arrange(desc(website_words))

        
frequency
```

